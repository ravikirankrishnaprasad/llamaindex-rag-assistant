# ğŸ“š LlamaIndex RAG Assistant

An **LLM-powered conversational assistant** built with **Retrieval Augmented Generation (RAG)** using **LlamaIndex, OpenAI, and Chainlit**.  
This project demonstrates how to enhance Large Language Models with external knowledge sources (Wikipedia in this case) to deliver accurate, context-aware responses.

---

## ğŸš€ Features
- ğŸ” **RAG Integration** â€“ Combines OpenAI LLMs with LlamaIndex for retrieval-augmented responses.  
- ğŸ“– **Wikipedia Knowledge Access** â€“ Augments chatbot knowledge with selected Wikipedia pages.  
- ğŸ’¬ **Conversational Interface** â€“ Built with Chainlit for an interactive chat UI.  
- ğŸ§  **Context-Aware Answers** â€“ Reduces hallucinations by grounding responses in retrieved documents.  

---

## ğŸ› ï¸ Tech Stack
- **[OpenAI](https://platform.openai.com/)** â€“ Large Language Model  
- **[LlamaIndex](https://www.llamaindex.ai/)** â€“ Retrieval-Augmented Generation framework  
- **[Chainlit](https://docs.chainlit.io/)** â€“ Conversational UI for LLM apps  
- **Python 3.10+** â€“ Core programming language  

---

## ğŸ“‚ Project Structure
```
llamaindex-rag-assistant/
â”‚â”€â”€ data/                # Wikipedia page(s) stored locally or via API
â”‚â”€â”€ app.py               # Main Chainlit application
â”‚â”€â”€ rag_engine.py        # RAG pipeline with LlamaIndex
â”‚â”€â”€ requirements.txt     # Python dependencies
â”‚â”€â”€ LICENSE              # License file (Apache 2.0)
â”‚â”€â”€ README.md            # Project documentation
```

---

## âš™ï¸ Installation

1. **Clone the repository**  
```bash
git clone https://github.com/<your-username>/llamaindex-rag-assistant.git
cd llamaindex-rag-assistant
```

2. **Create virtual environment & install dependencies**  
```bash
python3 -m venv venv
source venv/bin/activate   # On Mac/Linux
venv\Scriptsctivate      # On Windows

pip install -r requirements.txt
```

3. **Set your OpenAI API Key**  
```bash
export OPENAI_API_KEY="your-api-key"   # Mac/Linux
setx OPENAI_API_KEY "your-api-key"     # Windows
```

---

## â–¶ï¸ Usage

Run the chatbot with:
```bash
chainlit run app.py
```

Then open [http://localhost:8000](http://localhost:8000) in your browser to start chatting with the assistant.

---

## ğŸ“– Example Use Case
**Query:**  
```
Tell me about the life of Nikola Tesla.
```

**Response:**  
> The assistant retrieves facts from the selected Wikipedia page on **Nikola Tesla**, and the LLM generates a conversational, accurate response grounded in that content.

---

## ğŸ¯ Learning Objectives
- Understand how to build **RAG pipelines** with LlamaIndex.  
- Explore conversational interfaces with **Chainlit**.  
- Learn how to **augment LLMs with external knowledge sources**.  

---

## ğŸ—ï¸ Future Enhancements
- Add support for **multiple data sources** (PDFs, docs, websites, enterprise data).  
- Integrate with **vector databases** (Pinecone, Weaviate, FAISS).  
- Extend UI with **session memory & conversation history**.  
- Deploy to **cloud environments** (AWS/GCP/Azure).  

---

## ğŸ“œ License
This project is licensed under the **Apache License 2.0** - see the [LICENSE](./LICENSE) file for details.

---

## ğŸ‘¨â€ğŸ’» Author
**Ravikiran**  
Development Architect | AI/ML & Generative AI Enthusiast  
